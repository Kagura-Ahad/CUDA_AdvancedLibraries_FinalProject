This capstone project demonstrates my learning journey in GPU computing, where I did handson problem-solving and iterative refinement. 

I started this project with a half-written proof-of-concept GPU-accelerated image processing application using CuPy, but quickly recognized the need for a more production-ready solution, so I converted the entire codebase from CuPy to PyTorch, which involved learning how to translate GPU operations between different frameworks, understanding device placement, and adapting to PyTorch's tensor API while maintaining equivalent functionality. 

My coding practice involves always including comprehensive type hints to satisfy strict type checking requirement of VSCode, so I learned how to properly annotate function signatures, work with NumPy's typing system, and handle union types for device specifications as well. 

When I ran the converted code from CuPy to PyTorch, I encountered a runtime error related to tensor dimensionality in PyTorch's padding operations, which taught me about the framework's expectations for batch and channel dimensions, so I understood how to reshape tensors appropriately before and after operations. 

The most significant learning moment came when I noticed my GPU code was taking an unexpectedly long time to execute (90+ seconds for simple operations), which led to the discovery that my implementation was using inefficient nested Python loops that performed serial operations on GPU tensors rather than leveraging true parallelized GPU computation. This revelation prompted me to learn about PyTorch's built-in convolution operations (F.conv2d) and understand the fundamental difference between touching GPU memory in a loop versus launching optimized CUDA kernels, ultimately reducing processing times from over 90 seconds to under 1 second by replacing manual pixel-by-pixel iteration with vectorized GPU operations. Through this step-by-step debugging and optimization process, I gained practical experience in GPU programming paradigms, learned to identify performance bottlenecks, and developed an understanding of how to properly utilize GPU frameworks to achieve genuine parallel acceleration rather than merely moving data to GPU memory.